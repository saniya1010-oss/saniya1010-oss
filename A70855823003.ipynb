{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWBAYelyRr1c"
   },
   "source": [
    "# <center>Predicting and Visualizing the Output Product Molecules of Organic Reactions using Pretrained Transformer Models and RDKit</center>\n",
    "\n",
    "### Table of contents:\n",
    "\n",
    "#### 1. [Installing the Packages](#first-bullet)\n",
    "#### 2. [Data Preprocessing](#second-bullet)\n",
    "#### 3. [Vocabulary for OpenNMT-py](#third-bullet)\n",
    "#### 4. [Training the model](#fourth-bullet)\n",
    "#### 5. [Testing the model](#fifth-bullet)\n",
    "#### 6. [Performance measures of the model](#sixth-bullet)\n",
    "#### 7. [Comparison with Previous Work](#seventh-bullete)\n",
    "#### 8. [Conclusions & Results](#eighth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and Importing the Packages <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "We start by installing OpenNMT-py version 2.2.0, which is an open-source (MIT) neural machine translation framework and RDKit, a liabrary to Parse SMILES stings, visualize molecules, etc., followed by pytorch with CUDA to move model tensors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ngSQ6PEECKts"
   },
   "source": [
    "pip install rdkit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install OpenNMT-py==2.2.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZF5nnY_QCRlM"
   },
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "import os\n",
    "import yaml\n",
    "import torch #library for deep learning frameworks\n",
    "from rdkit import RDLogger # optional step to disable warnings of RDkit)\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72nKtCqZRr12"
   },
   "source": [
    "# Data Preprocessing <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "1. [Loading the Datasets](#firstsub-bullet)\n",
    "2. [Exploratory Data Analysis](#secondsub-bullet)\n",
    "3. [Splitting the Dataset](#thirdsub-bullet)\n",
    "4. [Canonicalizing the Datast](#fourthsub-bullet)\n",
    "5. [Removing the Atom Maps](#fifthsub-bullet)\n",
    "6. [Tokenizing the Dataset](#sixthsub-bullet)\n",
    "7. [Preparing the DataFrames](#seventhsub-bullet)\n",
    "8. [Shuffling and Saving](#eighthsub-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the datasets: Exploratory data analysis <a class=\"anchor\" id=\"firstsub-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mpXvhvA5C8Zw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id  class                      reactants>reagents>production\n",
      "0       US05849732      6  [NH:1]([CH2:2][CH2:3][CH2:4][CH2:5][C@@H:6]([C...\n",
      "1  US20120114765A1      2  [C:1](=[O:2])([c:3]1[cH:4][c:5]([N+:6](=[O:7])...\n",
      "2     US08003648B2      1  [CH3:44][CH2:45][NH:46][CH2:47][CH3:48].[CH:1]...\n",
      "3     US09045475B2      1  [C:1]([CH2:2][F:3])([CH2:4][F:5])=[O:65].[CH3:...\n",
      "4     US08188098B2      2  [C:1](=[O:2])([O:3][CH:4]1[CH2:5][CH2:6][CH2:7... \n",
      "\n",
      "                 id  class                      reactants>reagents>production\n",
      "0     US08329716B2      5  [C:1](=[O:2])([C:3]([F:4])([F:5])[F:6])[O:27][...\n",
      "1       US06051718      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...\n",
      "2     US07504410B2      5  [C:1](=[O:2])([C:3]([F:4])([F:5])[F:6])[O:19][...\n",
      "3       US04960769      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...\n",
      "4  US20110092505A1      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7... \n",
      "\n",
      "                 id  class                      reactants>reagents>production\n",
      "0     US07928231B2      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...\n",
      "1  US20090192322A1      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...\n",
      "2  US20080146614A1     10  [Br:1][N:35]1[C:30](=[O:29])[CH2:31][CH2:32][C...\n",
      "3  US20120207729A1      5  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...\n",
      "4  US20070003539A1      5  [C:1](=[O:2])([C:3]([F:4])([F:5])[F:6])[O:23][...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_test.csv\")\n",
    "val = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_val.csv\")\n",
    "\n",
    "#a glimpse of the datasets\n",
    "print(train.head(),'\\n\\n', val.head(), '\\n\\n', test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis <a class=\"anchor\" id=\"secondsub-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train dataset:\t(40008, 3)\n",
      "Shape of the validation dataset:\t(5001, 3)\n",
      "Shape of the test dataset:\t(5007, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the train dataset:\\t{train.shape}\\nShape of the validation dataset:\\t{val.shape}\\nShape of the test dataset:\\t{test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info of the train dataset:\t              class\n",
      "count  40008.000000\n",
      "mean       3.397570\n",
      "std        2.504438\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        6.000000\n",
      "max       10.000000\n",
      "Info of the validation dataset:\t             class\n",
      "count  5001.000000\n",
      "mean      3.396921\n",
      "std       2.504121\n",
      "min       1.000000\n",
      "25%       1.000000\n",
      "50%       2.000000\n",
      "75%       6.000000\n",
      "max      10.000000\n",
      "Info of the test dataset:\t             class\n",
      "count  5007.000000\n",
      "mean      3.399441\n",
      "std       2.505572\n",
      "min       1.000000\n",
      "25%       1.000000\n",
      "50%       2.000000\n",
      "75%       6.000000\n",
      "max      10.000000\n"
     ]
    }
   ],
   "source": [
    "print(f'Info of the train dataset:\\t{train.describe()}\\nInfo of the validation dataset:\\t{val.describe()}\\nInfo of the test dataset:\\t{test.describe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the train dataset:\tIndex(['id', 'class', 'reactants>reagents>production'], dtype='object')\n",
      "Columns in the validation dataset:\tIndex(['id', 'class', 'reactants>reagents>production'], dtype='object')\n",
      "Columns in the test dataset:\tIndex(['id', 'class', 'reactants>reagents>production'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Columns in the train dataset:\\t{train.columns}\\nColumns in the validation dataset:\\t{val.columns}\\nColumns in the test dataset:\\t{test.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>reactants&gt;reagents&gt;production</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US05849732</td>\n",
       "      <td>6</td>\n",
       "      <td>[NH:1]([CH2:2][CH2:3][CH2:4][CH2:5][C@@H:6]([C...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US20120114765A1</td>\n",
       "      <td>2</td>\n",
       "      <td>[C:1](=[O:2])([c:3]1[cH:4][c:5]([N+:6](=[O:7])...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US08003648B2</td>\n",
       "      <td>1</td>\n",
       "      <td>[CH3:44][CH2:45][NH:46][CH2:47][CH3:48].[CH:1]...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US09045475B2</td>\n",
       "      <td>1</td>\n",
       "      <td>[C:1]([CH2:2][F:3])([CH2:4][F:5])=[O:65].[CH3:...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US08188098B2</td>\n",
       "      <td>2</td>\n",
       "      <td>[C:1](=[O:2])([O:3][CH:4]1[CH2:5][CH2:6][CH2:7...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50011</th>\n",
       "      <td>US20050019696A1</td>\n",
       "      <td>2</td>\n",
       "      <td>[C:1]([C:2](=[CH2:3])[CH3:4])(=[O:5])[Cl:19].[...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50012</th>\n",
       "      <td>US20030139425A1</td>\n",
       "      <td>1</td>\n",
       "      <td>[CH2:1]([c:2]1[cH:3][cH:4][c:5]([F:6])[cH:7][c...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50013</th>\n",
       "      <td>US05411980</td>\n",
       "      <td>1</td>\n",
       "      <td>[CH3:7][CH2:8][CH2:9][CH2:10][c:11]1[n:12][nH:...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50014</th>\n",
       "      <td>US04426381</td>\n",
       "      <td>6</td>\n",
       "      <td>[O:1]([C:2](=[O:3])[c:4]1[c:5]2[n:6]([c:7]3[cH...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>US20040067202A1</td>\n",
       "      <td>6</td>\n",
       "      <td>[O:1]([CH2:2][CH2:3][O:4][CH2:5][CH2:6][O:7][C...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50016 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  class  \\\n",
       "0           US05849732      6   \n",
       "1      US20120114765A1      2   \n",
       "2         US08003648B2      1   \n",
       "3         US09045475B2      1   \n",
       "4         US08188098B2      2   \n",
       "...                ...    ...   \n",
       "50011  US20050019696A1      2   \n",
       "50012  US20030139425A1      1   \n",
       "50013       US05411980      1   \n",
       "50014       US04426381      6   \n",
       "50015  US20040067202A1      6   \n",
       "\n",
       "                           reactants>reagents>production source  \n",
       "0      [NH:1]([CH2:2][CH2:3][CH2:4][CH2:5][C@@H:6]([C...  train  \n",
       "1      [C:1](=[O:2])([c:3]1[cH:4][c:5]([N+:6](=[O:7])...  train  \n",
       "2      [CH3:44][CH2:45][NH:46][CH2:47][CH3:48].[CH:1]...  train  \n",
       "3      [C:1]([CH2:2][F:3])([CH2:4][F:5])=[O:65].[CH3:...  train  \n",
       "4      [C:1](=[O:2])([O:3][CH:4]1[CH2:5][CH2:6][CH2:7...  train  \n",
       "...                                                  ...    ...  \n",
       "50011  [C:1]([C:2](=[CH2:3])[CH3:4])(=[O:5])[Cl:19].[...   test  \n",
       "50012  [CH2:1]([c:2]1[cH:3][cH:4][c:5]([F:6])[cH:7][c...   test  \n",
       "50013  [CH3:7][CH2:8][CH2:9][CH2:10][c:11]1[n:12][nH:...   test  \n",
       "50014  [O:1]([C:2](=[O:3])[c:4]1[c:5]2[n:6]([c:7]3[cH...   test  \n",
       "50015  [O:1]([CH2:2][CH2:3][O:4][CH2:5][CH2:6][O:7][C...   test  \n",
       "\n",
       "[50016 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['source'] = 'train'\n",
    "val['source'] = 'val'\n",
    "test['source'] = 'test'\n",
    "\n",
    "df_all = pd.concat([train, val, test], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the datsets are open source, they are pretty much clean and don't have null values. We have explored the datasets as per our requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Dataset <a class=\"anchor\" id=\"thirdsub-bullet\"></a>\n",
    "\n",
    "As shown the dataset has three columns having the id of the reaction, the class, i.e., the type of the reaction, be it elimination, substitution encoded, along with the overall reaction. It's better for the reaction to be split into their components i.e., reactants, reagents and products for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'class', 'reactants>reagents>production', 'source', 'reactants',\n",
      "       'reagents', 'products'],\n",
      "      dtype='object') \n",
      " Index(['id', 'class', 'reactants>reagents>production', 'source', 'reactants',\n",
      "       'reagents', 'products'],\n",
      "      dtype='object') \n",
      " Index(['id', 'class', 'reactants>reagents>production', 'source', 'reactants',\n",
      "       'reagents', 'products'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train[['reactants', 'reagents', 'products']] = train['reactants>reagents>production'].str.split('>', expand = True)\n",
    "test[['reactants', 'reagents', 'products']] = test['reactants>reagents>production'].str.split('>', expand = True)\n",
    "val[['reactants', 'reagents', 'products']] = val['reactants>reagents>production'].str.split('>', expand = True)\n",
    "\n",
    "#checking the columns\n",
    "print(train.columns, '\\n', val.columns, '\\n', test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Canonicalizing the Data <a class=\"anchor\" id=\"fourthsub-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(smiles): # will raise an Exception if invalid SMILES\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "train['reactants'] = train['reactants'].apply(canonicalize)\n",
    "train['reagents'] = train['reagents'].apply(canonicalize)\n",
    "train['products'] = train['products'].apply(canonicalize)\n",
    "\n",
    "test['reactants'] = test['reactants'].apply(canonicalize)\n",
    "test['reagents'] = test['reagents'].apply(canonicalize)\n",
    "test['products'] = test['products'].apply(canonicalize)\n",
    "\n",
    "val['reactants'] = val['reactants'].apply(canonicalize)\n",
    "val['reagents'] = val['reagents'].apply(canonicalize)\n",
    "val['products'] = val['products'].apply(canonicalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removing the Atom maps <a class=\"anchor\" id=\"fifthsub-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_atommapping(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom.SetAtomMapNum(0)\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "train['reactants'] = train['reactants'].apply(remove_atommapping)\n",
    "train['reagents'] = train['reagents'].apply(remove_atommapping)\n",
    "train['products'] = train['products'].apply(remove_atommapping)\n",
    "\n",
    "test['reactants'] = test['reactants'].apply(remove_atommapping)\n",
    "test['reagents'] = test['reagents'].apply(remove_atommapping)\n",
    "test['products'] = test['products'].apply(remove_atommapping)\n",
    "\n",
    "val['reactants'] = val['reactants'].apply(remove_atommapping)\n",
    "val['reagents'] = val['reagents'].apply(remove_atommapping)\n",
    "val['products'] = val['products'].apply(remove_atommapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnNXgo3rRr2G"
   },
   "source": [
    "### 6. Tokenizing the Dataset <a class=\"anchor\" id=\"sixthsub-bullet\"></a>\n",
    "\n",
    "To be able to train a language model, we need to split the strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0R_3bNN9Rr2H"
   },
   "outputs": [],
   "source": [
    "REGEX_TOKENIZER =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "def tokenize(smiles):\n",
    "    return ' '.join(smiles)\n",
    "\n",
    "train['token_reactants'] = train['reactants'].apply(tokenize)\n",
    "train['token_reagents'] = train['reagents'].apply(tokenize)\n",
    "train['token_products'] = train['products'].apply(tokenize)\n",
    "\n",
    "test['token_reactants'] = test['reactants'].apply(tokenize)\n",
    "test['token_reagents'] = test['reagents'].apply(tokenize)\n",
    "test['token_products'] = test['products'].apply(tokenize)\n",
    "\n",
    "val['token_reactants'] = val['reactants'].apply(tokenize)\n",
    "val['token_reagents'] = val['reagents'].apply(tokenize)\n",
    "val['token_products'] = val['products'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Preparing the Data Frames <a class=\"anchor\" id=\"seventhsub-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Id': train['id'], \n",
    "                         'Class': train['class'], \n",
    "                         'Tokenized Reactants': train['token_reactants'],\n",
    "                         'Tokenized Products': train['token_products'],\n",
    "                        'Overall Reaction': train['reactants>reagents>production']})\n",
    "print(f\"The training set contains {train_df.shape[0]} reactions.\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Id': test['id'], \n",
    "                        'Class': test['class'],\n",
    "                        'Tokenized Reactants': test['token_reactants'],\n",
    "                        'Tokenized Products': test['token_products'],\n",
    "                       'Overall Reaction': test['reactants>reagents>production']})\n",
    "print(f\"The training set contains {test_df.shape[0]} reactions.\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({'Id': val['id'],\n",
    "                       'Class': val['class'],\n",
    "                       'Tokenized Reactants': val['token_reactants'],\n",
    "                       'Tokenized Products': val['token_products'],\n",
    "                      'Overall Reaction': val['reactants>reagents>production']})\n",
    "print(f\"The training set contains {val_df.shape[0]} reactions.\")\n",
    "val_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and saving the datasets <a class=\"anchor\" id=\"eighthsub-bullet\"></a>\n",
    "\n",
    "The dataset contains different types of reactions arranged in a ordered manner (as shown the snippet has same type of reaction i.e., 5) hence, without shuffling model might learn patterns that are not generalizable. Shuffling ensures that each training batch has a variety of reaction types, reactants, and complexities. This helps the model learn general rules of reactivity, and avoid overfitting. After shuffling it can be seen the dataset is random from the classof the reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rn = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_rn = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_rn = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_rn.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rn['Tokenized Reactants'].to_csv(\"uspto_50k_train_reactants.txt\", index=False, header=False)\n",
    "train_rn['Tokenized Products'].to_csv(\"uspto_50k_train_products.txt\", index=False, header=False)\n",
    "\n",
    "test_rn['Tokenized Reactants'].to_csv(\"uspto_50k_test_reactants.txt\", index=False, header=False)\n",
    "test_rn['Tokenized Products'].to_csv(\"uspto_50k_test_products.txt\", index=False, header=False)\n",
    "\n",
    "val_rn['Tokenized Reactants'].to_csv(\"uspto_50k_val_reactants.txt\", index=False, header=False)\n",
    "val_rn['Tokenized Products'].to_csv(\"uspto_50k_val_products.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary for OpenNMT-py <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Open-NMT-py requires to know, what tokens exists (here its sequence of atoms and bonds), how long can the input/out sequence be, where the training, validation data is, and how to handle tokenization to adapt to any language translation.\n",
    "\n",
    "Source: https://opennmt.net/OpenNMT-py/options/build_vocab.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'save_data': 'uspto_50k_run',\n",
    "    'src_vocab': 'uspto_50k_run/vocab.src',\n",
    "    'tgt_vocab': 'uspto_50k_run/vocab.tgt',\n",
    "    'overwrite': True,\n",
    "    'share_vocab': True,\n",
    "    'data': {\n",
    "        'corpus-1': {\n",
    "            'path_src': 'uspto_50k_train_reactants.txt',\n",
    "            'path_tgt': 'uspto_50k_train_products.txt',\n",
    "        },\n",
    "        'valid': {\n",
    "            'path_src': 'uspto_50k_val_reactants.txt',\n",
    "            'path_tgt': 'uspto_50k_val_products.txt',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create folder and save YAML file\n",
    "folder = 'example_run'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "with open(os.path.join(folder, 'run_config.yaml'), 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"run_config.yaml created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(\"example_run/run_config.yaml\") #checking if the config file has been created or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onmt.bin.build_vocab -config example_run/run_config.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.exists(\"uspto_50k_run/vocab.src\"))  # Should be True\n",
    "print(os.path.exists(\"uspto_50k_run/vocab.tgt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCA5W1xqRr2P"
   },
   "source": [
    "# Training the model <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Transformer model from scratch on CPU can take hours. Considering this project an experimentation, we will create a small transformer model, with hit and trial method to choose the right train_steps and choose the right one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "world_size: 1\n",
    "gpu_ranks: [0]  # Use CPU by removing or modifying this\n",
    "train_steps: 20000\n",
    "save_model: example_run/model\n",
    "save_checkpoint_steps: 1000\n",
    "batch_size: 64\n",
    "valid_steps: 10000\n",
    "report_every: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_train -config example_run/run_config.yaml \\\n",
    "-seed 42 \\\n",
    "-batch_type tokens -batch_size 64 \\\n",
    "-accum_count 2 \\\n",
    "-optim adam -adam_beta2 0.998 \\\n",
    "-decay_method noam -warmup_steps 4000 \\\n",
    "-learning_rate 1 \\\n",
    "-label_smoothing 0.0 \\\n",
    "-layers 2 -rnn_size 256 -word_vec_size 256 \\\n",
    "-encoder_type transformer -decoder_type transformer \\\n",
    "-dropout 0.1 -position_encoding \\\n",
    "-share_embeddings \\\n",
    "-transformer_ff 1024 \\\n",
    "-tensorboard True -tensorboard_log_dir log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[INFO] Step 40000/40000; acc: 48.1; ppl: 32.5; xent: 44.56; lr: 0.00038; ...e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[INFO] Saving checkpoint to example_run/model_step_40000.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[INFO] Training complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCA5W1xqRr2P"
   },
   "source": [
    "# Performance measures of the model <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate \\\n",
    "  -model model_step_20000.pt \\\n",
    "  -src path/to/val_src.txt \\\n",
    "  -tgt path/to/val_tgt.txt \\\n",
    "  -output example_run/predictions_val.txt \\\n",
    "  -gpu 0 \\\n",
    "  -beam_size 10 \\\n",
    "  -n_best 5 \\\n",
    "  -max_length 380 \\\n",
    "  -batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load ground truth and predictions\n",
    "with open(\"path/to/val_tgt.txt\") as f:\n",
    "    targets = [line.strip() for line in f]\n",
    "\n",
    "with open(\"path/to/val_src.txt\") as f:\n",
    "    precursors = [line.strip() for line in f]\n",
    "\n",
    "n_best = 5\n",
    "predictions = [[] for _ in range(n_best)]\n",
    "\n",
    "with open(\"example_run/predictions_val.txt\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        predictions[i % n_best].append(line.strip())\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"target\": targets,\n",
    "    \"precursors\": precursors\n",
    "})\n",
    "\n",
    "for i in range(n_best):\n",
    "    df[f\"prediction_{i+1}\"] = predictions[i]\n",
    "\n",
    "# Canonicalize SMILES\n",
    "def canonicalize(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for i in range(n_best):\n",
    "    df[f\"canonical_prediction_{i+1}\"] = df[f\"prediction_{i+1}\"].apply(canonicalize)\n",
    "\n",
    "# Calculate top-n accuracy\n",
    "def get_rank(row):\n",
    "    for i in range(n_best):\n",
    "        if row[\"target\"] == row[f\"canonical_prediction_{i+1}\"]:\n",
    "            return i + 1\n",
    "    return None\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"rank\"] = df.progress_apply(get_rank, axis=1)\n",
    "\n",
    "# Report top-n accuracy\n",
    "for i in range(1, n_best + 1):\n",
    "    correct = df[\"rank\"].apply(lambda x: x is not None and x <= i).sum()\n",
    "    print(f\"Top-{i} Accuracy: {correct / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Report invalid SMILES\n",
    "for i in range(n_best):\n",
    "    invalid = df[f\"canonical_prediction_{i+1}\"].isna().sum()\n",
    "    print(f\"Invalid SMILES in Top-{i+1}: {invalid}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test_reaction_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
